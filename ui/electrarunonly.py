# -*- coding: utf-8 -*-
"""ELECTRARunOnly.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AN9_0B4Y0WkG4B2irEqbu6JWwjVzaJfs
"""

# ! pip install transformers
# ! pip install tokenizers

import json
import os
import random
import numpy as np
import collections
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import transformers
from transformers import TFElectraModel, ElectraTokenizerFast

# from google.colab import drive
# from google.colab import drive
# drive.mount('/content/gdrive')

path = "Q_A projesi/"
models_path = path + "models/"
MODEL_NAME = "dbmdz/electra-base-turkish-cased-discriminator"
save_path = models_path + "electra-base-turkish-cased-discriminator"

MAX_LEN = 384
configuration = transformers.ElectraConfig()


def create_model():
    ## ELECTRA encoder
    # encoder = TFElectraModel.from_pretrained(MODEL_NAME)
    encoder = TFElectraModel.from_pretrained(save_path + "/")

    # QA model
    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)
    token_type_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)
    embedding = encoder.electra(input_ids,
                                token_type_ids=token_type_ids,
                                attention_mask=attention_mask)[0]

    start_logits = layers.Dense(1, name="start_logit", use_bias=False)(embedding)
    start_logits = layers.Flatten()(start_logits)

    end_logits = layers.Dense(1, name="end_logit", use_bias=False)(embedding)
    end_logits = layers.Flatten()(end_logits)

    start_probs = layers.Activation(keras.activations.softmax)(start_logits)
    end_probs = layers.Activation(keras.activations.softmax)(end_logits)

    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[start_probs, end_probs],
    )

    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=5e-5)
    model.compile(optimizer=optimizer, loss=[loss, loss])
    return model


# print(models_path+MODEL_NAME)

tokenizer = ElectraTokenizerFast(vocab_file=save_path + "/vocab.txt", do_lower_case=False)
# tokenizer = ElectraTokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=False)
# tokenizer = ElectraTokenizerFast.from_pretrained(save_path+"/vocab.txt", do_lower_case=False) #path to a directoy istiyor

model = create_model()
# model.summary()

model.load_weights(
    save_path + "/weights/" + "electra-base-turkish-cased-discriminator" + "_seqlen512_batch64_epochs10_weights.h5")


class DataElement:
    def __init__(self, question, context):
        self.question = question
        self.context = context

    def preprocess(self):
        # tokenize context   
        tokenized_context = tokenizer(self.context, return_offsets_mapping=True)

        # tokenize question
        tokenized_question = tokenizer(self.question, return_offsets_mapping=True)

        # create inputs       
        input_ids = tokenized_context['input_ids'] + tokenized_question['input_ids'][1:]
        token_type_ids = [0] * len(tokenized_context['input_ids']) + [1] * len(tokenized_question['input_ids'][1:])

        attention_mask = [1] * len(input_ids)

        # padding for equal lenght sequence
        padding_length = MAX_LEN - len(input_ids)
        if padding_length > 0:  # pad
            input_ids = input_ids + ([0] * padding_length)
            attention_mask = attention_mask + ([0] * padding_length)  # len(input) [1] + padding [0]
            token_type_ids = token_type_ids + ([0] * padding_length)  # context [0] + question [1] + padding [0]
        elif padding_length < 0:
            return

        self.input_ids = input_ids
        self.token_type_ids = token_type_ids
        self.attention_mask = attention_mask
        self.context_token_to_char = tokenized_context['offset_mapping']


def create_input_targets(element):
    dataset_dict = {
        "input_ids": [],
        "token_type_ids": [],
        "attention_mask": [],
    }

    i = 0

    for key in dataset_dict:
        dataset_dict[key].append(getattr(element, key))

    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict["input_ids"],
        dataset_dict["token_type_ids"],
        dataset_dict["attention_mask"],
    ]
    return x


def predict_answer(question, context):
    element = DataElement(question, context)
    element.preprocess()

    x = create_input_targets(element)

    pred_start, pred_end = model.predict(x)

    start = np.argmax(pred_start)
    end = np.argmax(pred_end)
    offsets = element.context_token_to_char

    pred_char_start = offsets[start][0]
    output_start = 0
    output_end = 0
    if end < len(offsets):
        pred_char_end = offsets[end][1]
        pred_ans = element.context[pred_char_start:pred_char_end]
    else:
        pred_ans = element.context[pred_char_start:]

    # print("question: {}\n\npredicted_answer: {}\n\ncontext: {}".format(element.question, pred_ans, element.context))
    output_start = pred_char_start
    output_end = pred_char_end
    result = {"question": element.question,
              "predicted_answer": pred_ans,
              "context": element.context,
              "output_start": output_start,
              "output_end": output_end}

    return result


def get_result(context, question):
    my_context = context
    my_question = question

    result = predict_answer(my_question, my_context)
    return result['predicted_answer'],result['output_start'],result['output_end']
